{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkashSinghrajpoot/text-to-image-generation-modle/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textwrap import wrap\n",
        "\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"dark\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "DFZRx7_oyFJw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/kaggle/input/flickr8k/Images'\n",
        "data = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "ZpCwJoYGyTS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readImage(path,img_size=224):\n",
        "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "\n",
        "    return img\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(15):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
        "        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
        "        plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "fVo7LYohyaIo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(data):\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n",
        "    return data"
      ],
      "metadata": {
        "id": "2-gVOfaUypKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
        " 'startseq girl going into wooden building endseq',\n",
        " 'startseq little girl climbing into wooden playhouse endseq',\n",
        " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
        " 'startseq little girl in pink dress going into wooden cabin endseq',\n",
        " 'startseq black dog and spotted dog are fighting endseq',\n",
        " 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n",
        " 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n",
        " 'startseq two dogs of different breeds looking at each other on the road endseq',\n",
        " 'startseq two dogs on pavement moving toward each other endseq']"
      ],
      "metadata": {
        "id": "0GYEn0ivyyQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in captions)\n",
        "\n",
        "images = data['image'].unique().tolist()\n",
        "nimages = len(images)\n",
        "\n",
        "split_index = round(0.85*nimages)\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "train = data[data['image'].isin(train_images)]\n",
        "test = data[data['image'].isin(val_images)]\n",
        "\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "test.reset_index(inplace=True,drop=True)\n",
        "\n",
        "tokenizer.texts_to_sequences([captions[1]])[0]"
      ],
      "metadata": {
        "id": "7G8gec7qy7GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenseNet201()\n",
        "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "img_size = 224\n",
        "features = {}\n",
        "for image in tqdm(data['image'].unique().tolist()):\n",
        "    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "    img = np.expand_dims(img,axis=0)\n",
        "    feature = fe.predict(img, verbose=0)\n",
        "    features[image] = feature"
      ],
      "metadata": {
        "id": "9QbSWpWey_l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
        "                 vocab_size, max_length, features,shuffle=True):\n",
        "\n",
        "        self.df = df.copy()\n",
        "        self.X_col = X_col\n",
        "        self.y_col = y_col\n",
        "        self.directory = directory\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.features = features\n",
        "        self.shuffle = shuffle\n",
        "        self.n = len(self.df)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n",
        "        X1, X2, y = self.__get_data(batch)\n",
        "        return (X1, X2), y\n",
        "\n",
        "    def __get_data(self,batch):\n",
        "\n",
        "        X1, X2, y = list(), list(), list()\n",
        "\n",
        "        images = batch[self.X_col].tolist()\n",
        "\n",
        "        for image in images:\n",
        "            feature = self.features[image][0]\n",
        "\n",
        "            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n",
        "            for caption in captions:\n",
        "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "                for i in range(1,len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
        "                    X1.append(feature)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "        return X1, X2, y\n",
        "\n",
        "\n",
        "train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
        "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
        "\n",
        "validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
        "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"
      ],
      "metadata": {
        "id": "kRBpFmEEzEoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "input1 = Input(shape=(1920,))\n",
        "input2 = Input(shape=(max_length,))\n",
        "\n",
        "img_features = Dense(256, activation='relu')(input1)\n",
        "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
        "\n",
        "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
        "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
        "sentence_features = LSTM(256)(merged)\n",
        "x = Dropout(0.5)(sentence_features)\n",
        "x = add([x, img_features])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "caption_model = Model(inputs=[input1,input2], outputs=output)\n",
        "caption_model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the model checkpoint\n",
        "model_name = \"model.keras\"  # Update the extension to .keras\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_name,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            patience=3,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.2,\n",
        "                                            min_lr=0.00000001)\n",
        "\n",
        "history = caption_model.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
      ],
      "metadata": {
        "id": "JSPzkd9zzSkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1mqsRD9zobZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Save the feature extractor model\n",
        "feature_extractor.save(\"feature_extractor.keras\")\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# load save files\n",
        "model_path = \"model.keras\"\n",
        "tokenizer_path = \"tokenizer.pkl\"\n",
        "feature_extractor_path = \"feature_extractor.keras\"\n",
        "\n",
        "\n",
        "def generate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path, max_length=34, img_size=224):\n",
        "    # Load the trained models and tokenizer\n",
        "    caption_model = load_model(model_path)\n",
        "    feature_extractor = load_model(feature_extractor_path)\n",
        "\n",
        "    with open(tokenizer_path, \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    # Preprocess the image\n",
        "    img = load_img(image_path, target_size=(img_size, img_size))\n",
        "    img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    image_features = feature_extractor.predict(img, verbose=0)  # Extract image features\n",
        "\n",
        "    # Generate the caption\n",
        "    in_text = \"startseq\"\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = caption_model.predict([image_features, sequence], verbose=0)\n",
        "        yhat_index = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat_index, None)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += \" \" + word\n",
        "        if word == \"endseq\":\n",
        "            break\n",
        "    caption = in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n",
        "\n",
        "    # Display the image with the generated caption\n",
        "    img = load_img(image_path, target_size=(img_size, img_size))\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(caption, fontsize=16, color='blue')\n",
        "    plt.show()\n",
        "# Example usage\n",
        "image_path = \"/kaggle/input/flickr8k/Images/110595925_f3395c8bd6.jpg\"  # Replace with the path to the input image\n",
        "generate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path)\n"
      ],
      "metadata": {
        "id": "end5lFUKzyZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/kaggle/input/flickr8k/Images/116409198_0fe0c94f3b.jpg\"  # Replace with the path to the input image\n",
        "generate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path)"
      ],
      "metadata": {
        "id": "FBmdaYh-0TeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tt4ADhgI06yY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}